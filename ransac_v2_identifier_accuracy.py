# -*- coding: utf-8 -*-
"""RANSAC_v2 Identifier Accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_F2EOa0JUUIxhOpom2NkIscwXnrWzQl
"""

from scipy.stats import entropy
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras import regularizers
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv2D, GlobalMaxPooling2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import random
from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow as tf
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "6"  # (xxxx is your specific GPU ID)

print("HELLO WORLD")


# method to add noisy labels to data

def corruptData(trainY, noisePercentage):
    # create copies of labels
    copyTrainY = trainY.copy()

    # calculate number of samples to be made noisy
    numberNoisyTrain = int(noisePercentage * len(copyTrainY))

    # generate indexes to swap
    trainYSwitchIndexes = random.sample(
        range(0, len(copyTrainY)), numberNoisyTrain)

    # generate new classes not equal to original for training and switch class
    for i in range(len(trainYSwitchIndexes)):
        label = random.choice(range(10))
        # find label that isn't the same
        while label == trainY[trainYSwitchIndexes[i]]:
            label = random.choice(range(10))
        # switch label
        copyTrainY[trainYSwitchIndexes[i]] = label

    return copyTrainY


def splitTrainingData(trainX, trainY, splitPercentage):
    # get number of elements to split
    numberSplit = int(splitPercentage * len(trainX))
    # generate indexes to split
    indexes = list(range(len(trainX)))
    beforeSplitIndexes = random.sample(range(0, len(trainX)), numberSplit)
    afterSplitIndexes = list(set(indexes)-set(beforeSplitIndexes))

    # make new arrays
    firstTrainX = []
    firstTrainY = []
    secondTrainX = []
    secondTrainY = []

    # add each data sample to corresponding list
    for index in beforeSplitIndexes:
        firstTrainX.append(trainX[index])
        firstTrainY.append(trainY[index])
    for index in afterSplitIndexes:
        secondTrainX.append(trainX[index])
        secondTrainY.append(trainY[index])
    return np.array(firstTrainX), np.array(firstTrainY), np.array(secondTrainX), np.array(secondTrainY), beforeSplitIndexes, afterSplitIndexes


def trainModel(trainX, trainY, n):
    # pre-train the model

    # number of classes
    K = 10

    # calculate total number of classes
    # for output layer
    print("number of classes:", K)

    # Build the model using the functional API
    # input layer
    i = Input(shape=trainX[0].shape)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)
    x = BatchNormalization()(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    if n == 2:
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2, 2))(x)

    if n == 3:
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2, 2))(x)

        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2, 2))(x)

    x = Flatten()(x)
    x = Dropout(0.2)(x)

    # Hidden layer
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.2)(x)

    # last hidden layer i.e.. output layer
    x = Dense(K, activation='softmax')(x)

    model = Model(i, x)

    # model description
    # model.summary()

    # Compile
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Fit
    r = model.fit(trainX, trainY, epochs=20)

    return model


def makeConfidentTrainingSets(model, firstTrainX, firstTrainY, secondTrainX, secondTrainY, perEntropy, perPeak, beforeSplitIndexes, afterSplitIndexes):
    newTrainX = []
    newTrainY = []
    confidentIndexes = []
    # find confident samples from first training set
    # obtain probability distribution of classes for each sample after the split and calculate its entropy
    # make predictions
    firstTrainXPredictions = model.predict(firstTrainX)
    # find entropy and peak value for every sample
    firstTrainXEntropies = []
    firstTrainXPeakValues = []
    for sample in firstTrainXPredictions:
        # calculate entropy
        sampleEntropy = entropy(sample)
        # calculate peak value
        probSorted = sorted(sample)
        peakValue = probSorted[0]/probSorted[1]

        firstTrainXEntropies.append(sampleEntropy)
        firstTrainXPeakValues.append(peakValue)

    # set NANs to 0
    firstTrainXPeakValues = np.array(firstTrainXPeakValues)
    firstTrainXPeakValues[np.isnan(firstTrainXPeakValues)] = 0
    # calculate mean of entropy and peak value
    meanEntropy = np.mean(firstTrainXEntropies)
    meanPeakValue = np.mean(firstTrainXPeakValues)

    # entropy and peak value hyperparameter
    entropyVal = perEntropy*meanEntropy
    peakVal = perPeak*meanPeakValue

    # obtain samples that were correctly predicted and fall under the threshold for entropy and peak value
    for i in range(len(firstTrainXPredictions)):
        probDist = firstTrainXPredictions[i]
        predictedClass = np.argmax(probDist)

        # if confident add to list
        if predictedClass == firstTrainY[i] and firstTrainXEntropies[i] <= entropyVal and firstTrainXPeakValues[i] > peakVal:
            newTrainX.append(firstTrainX[i])
            newTrainY.append(firstTrainY[i])
            confidentIndexes.append(beforeSplitIndexes[i])

    # find confident samples from unused training set
    # obtain probability distribution of classes for each sample after the split and calculate its entropy
    # make predictions
    secondTrainXPredictions = model.predict(secondTrainX)
    # find entropy and peak value for every sample
    secondTrainXEntropies = []
    secondTrainXPeakValues = []
    for sample in secondTrainXPredictions:
        # calculate entropy
        sampleEntropy = entropy(sample)
        # calculate peak value
        probSorted = sorted(sample)
        peakValue = probSorted[0]/probSorted[1]

        secondTrainXEntropies.append(sampleEntropy)
        secondTrainXPeakValues.append(peakValue)

    # set NANs to 0
    secondTrainXPeakValues = np.array(secondTrainXPeakValues)
    secondTrainXPeakValues[np.isnan(secondTrainXPeakValues)] = 0

    # calculate mean of entropy and peak value
    meanEntropy = np.mean(secondTrainXEntropies)
    meanPeakValue = np.mean(secondTrainXPeakValues)

    # entropy and peak value hyperparameter
    entropyVal = perEntropy*meanEntropy
    peakVal = perPeak*meanPeakValue

    # obtain samples that were correctly predicted and fall under the threshold for entropy and peak value
    for i in range(len(secondTrainXPredictions)):
        probDist = secondTrainXPredictions[i]
        predictedClass = np.argmax(probDist)

        # if confident add to list
        if predictedClass == secondTrainY[i] and secondTrainXEntropies[i] <= entropyVal and secondTrainXPeakValues[i] > peakVal:
            newTrainX.append(secondTrainX[i])
            newTrainY.append(secondTrainY[i])
            confidentIndexes.append(afterSplitIndexes[i])

    # make plots
    sortedFirstTrainXEntropies = sorted(firstTrainXEntropies)
    sortedFirstTrainXIndices = list(range(len(firstTrainXEntropies)))

    plt.plot(sortedFirstTrainXIndices, sortedFirstTrainXEntropies)
    plt.show()

    return newTrainX, newTrainY, confidentIndexes


(trainX, trainY), (testX, testY) = cifar10.load_data()

# Normalize pixel values to be between 0 and 1
trainX, testX = trainX / 255.0, testX / 255.0

# flatten the label values
trainY, testY = trainY.flatten(), testY.flatten()

# corrupt data
noisePercentage = 0.25
trainYMislabeled = corruptData(trainY, noisePercentage)

# cleanModel = load_model('CleanModelTraining/ransac_clean.h5')
# upperBoundAccuracy = cleanModel.evaluate(testX, testY)[1]

# print(upperBoundAccuracy)

print("Num GPUs Available: ", len(
    tf.config.experimental.list_physical_devices('GPU')))

# collect best indexes over multiple models
bestIndexes = list(range(len(trainX)))
for i in range(5):
    # split data
    splitPercentage = .7
    firstTrainX, firstTrainY, secondTrainX, secondTrainY, beforeSplitIndexes, afterSplitIndexes = splitTrainingData(
        trainX, trainYMislabeled, splitPercentage)

    # train model used to identify confident samples
    confidenceModel = trainModel(firstTrainX, firstTrainY, 1)
    percentageOfEntropy = [0.25, .5, 1, 3]
    percentageOfPeak = [5, 3, 1, .5]
    for j in range(len(percentageOfEntropy)):
        perEntropy = percentageOfEntropy[j]
        perPeak = percentageOfPeak[j]

        # find samples that this model is confident on
        newTrainX, newTrainY, confidentIndexes = makeConfidentTrainingSets(
            confidenceModel, firstTrainX, firstTrainY, secondTrainX, secondTrainY, perEntropy, perPeak, beforeSplitIndexes, afterSplitIndexes)

        # add 1 to every confident image
        for index in confidentIndexes:
            bestIndexes[index] += 1

# make new datasets for the most confident samples
bestTrainX = []
bestTrainY = []

# sort and preserve index
bestSorted = np.argsort(bestIndexes)
bestSorted = bestSorted[::-1]

# calculate number of samples to use
numberCertain = int(0.5 * len(bestIndexes))

# take certain samples
for i in range(numberCertain):
    bestTrainX.append(trainX[bestSorted[i]])
    bestTrainY.append(trainYMislabeled[bestSorted[i]])

# run experiments
# train a new model on these confident samples
bestTrainX = np.array(bestTrainX)
bestTrainY = np.array(bestTrainY)
ransacModel = trainModel(bestTrainX, bestTrainY, 1)

# calculate accuracy of this model in using test data
accuracy = ransacModel.evaluate(testX, testY)[1]

print('This model has an accuracy of', accuracy, 'on the testing data.')
