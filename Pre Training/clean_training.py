# -*- coding: utf-8 -*-
"""Clean Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ztz6lAOgpL7rKrk_1bHqK3kq-2kEbfe
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
from data_utils import *

# %matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "6"  # (xxxx is your specific GPU ID)
config = tf.ConfigProto()
# config.gpu_options.per_process_gpu_memory_fraction = 0.1
config.gpu_options.allow_growth = True
tf.keras.backend.set_session(tf.Session(config=config))

# %load_ext autoreload
# %autoreload 2

# get data
cifar10_data = CIFAR10Data()
x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)

num_train = int(x_train.shape[0] * 0.9)
num_val = x_train.shape[0] - num_train
mask = list(range(num_train, num_train+num_val))
x_val = x_train[mask]
y_val = y_train[mask]

mask = list(range(num_train))
x_train = x_train[mask]
y_train = y_train[mask]

print('num train:%d num val:%d' % (num_train, num_val))
data = (x_train, y_train, x_val, y_val, x_test, y_test)

from ResNet import ResNet20ForCIFAR10
from keras import losses
from keras import optimizers

weight_decay = 1e-4
lr = 1e-1
num_classes = 10
resnet20 = ResNet20ForCIFAR10(input_shape=(32, 32, 3), classes=num_classes, weight_decay=weight_decay)
opt = tf.keras.optimizers.SGD(lr=lr, momentum=0.9, nesterov=False)
resnet20.compile(optimizer=opt,
                 loss=losses.categorical_crossentropy,
                 metrics=['accuracy'])
resnet20.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from cifar10_solver import *
# # from keras.callbacks import ReduceLROnPlateau
# from keras.callbacks import LearningRateScheduler
# 
# def lr_scheduler(epoch):
#     new_lr = lr
#     if epoch <= 91:
#         pass
#     elif epoch > 91 and epoch <= 137:
#         new_lr = lr * 0.1
#     else:
#         new_lr = lr * 0.01
#     print('new lr:%.2e' % new_lr)
#     return new_lr 
# 
# reduce_lr = LearningRateScheduler(lr_scheduler)
# # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,
# #                               patience=10, min_lr=1e-6, verbose=1)
# 
# solver = CIFAR10Solver(resnet20, data)
# history = solver.train(epochs=182, batch_size=128, data_augmentation=True, callbacks=[reduce_lr])

solver.test()

solver.model.save('pretrain_model.h5')